I1027 18:42:31.109504 40125 caffe.cpp:217] Using GPUs 5
I1027 18:42:31.511715 40125 caffe.cpp:222] GPU 5: GeForce GTX 1080 Ti
I1027 18:42:33.867410 40125 solver.cpp:63] Initializing solver from parameters: 
test_iter: 1
test_interval: 2000
base_lr: 0.0005
display: 50
max_iter: 6000000
lr_policy: "step"
gamma: 0.9
momentum: 0.9
weight_decay: 0.004
stepsize: 50000
snapshot: 10000
snapshot_prefix: "./models/mouth/1026+1025f"
solver_mode: GPU
device_id: 5
net: "examples/hand_reg/alimouth/traintest.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1027 18:42:33.869945 40125 solver.cpp:106] Creating training net from net file: examples/hand_reg/alimouth/traintest.prototxt
I1027 18:42:33.914746 40125 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/hand_reg/alimouth/traintest.prototxt
I1027 18:42:33.914822 40125 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1027 18:42:33.914979 40125 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer test-data
I1027 18:42:33.915293 40125 net.cpp:58] Initializing net from parameters: 
name: "hand_gesture_reg_mouth64"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "input-data"
  type: "Python"
  top: "data"
  top: "roi"
  include {
    phase: TRAIN
  }
  python_param {
    module: "pythonLayer"
    layer: "Data_Layer_train"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "conv1_bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "conv1_bn_scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "conv2_bn"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "conv2_bn_scale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "conv2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "conv3_bn"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "conv3_bn_scale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "conv4_bn"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "conv4_bn_scale"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "conv5_bn"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "conv5_bn_scale"
  type: "Scale"
  bottom: "conv5"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "conv5"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  inner_product_param {
    num_output: 128
    bias_term: false
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "ip1_bn"
  type: "BatchNorm"
  bottom: "ip1"
  top: "ip1"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "ip1_bn_scale"
  type: "Scale"
  bottom: "ip1"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu-ip1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "fullyconnected1"
  type: "InnerProduct"
  bottom: "ip1"
  top: "fullyconnected1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "smallRegressionLoss"
  type: "EuclideanLoss"
  bottom: "fullyconnected1"
  bottom: "roi"
  top: "RegressionLoss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
}
I1027 18:42:33.915524 40125 layer_factory.hpp:77] Creating layer input-data
I1027 18:42:36.907366 40125 net.cpp:100] Creating Layer input-data
I1027 18:42:36.907435 40125 net.cpp:408] input-data -> data
I1027 18:42:36.907490 40125 net.cpp:408] input-data -> roi
I1027 18:42:52.907781 40125 net.cpp:150] Setting up input-data
I1027 18:42:52.907891 40125 net.cpp:157] Top shape: 128 3 64 64 (1572864)
I1027 18:42:52.907907 40125 net.cpp:157] Top shape: 128 4 (512)
I1027 18:42:52.907918 40125 net.cpp:165] Memory required for data: 6293504
I1027 18:42:52.907954 40125 layer_factory.hpp:77] Creating layer conv1
I1027 18:42:52.908007 40125 net.cpp:100] Creating Layer conv1
I1027 18:42:52.908123 40125 net.cpp:434] conv1 <- data
I1027 18:42:52.908167 40125 net.cpp:408] conv1 -> conv1
I1027 18:42:52.912215 40125 net.cpp:150] Setting up conv1
I1027 18:42:52.912259 40125 net.cpp:157] Top shape: 128 12 32 32 (1572864)
I1027 18:42:52.912273 40125 net.cpp:165] Memory required for data: 12584960
I1027 18:42:52.912307 40125 layer_factory.hpp:77] Creating layer conv1_bn
I1027 18:42:52.912353 40125 net.cpp:100] Creating Layer conv1_bn
I1027 18:42:52.912370 40125 net.cpp:434] conv1_bn <- conv1
I1027 18:42:52.912418 40125 net.cpp:395] conv1_bn -> conv1 (in-place)
I1027 18:42:52.912714 40125 net.cpp:150] Setting up conv1_bn
I1027 18:42:52.912756 40125 net.cpp:157] Top shape: 128 12 32 32 (1572864)
I1027 18:42:52.912829 40125 net.cpp:165] Memory required for data: 18876416
I1027 18:42:52.912852 40125 layer_factory.hpp:77] Creating layer conv1_bn_scale
I1027 18:42:52.912890 40125 net.cpp:100] Creating Layer conv1_bn_scale
I1027 18:42:52.912904 40125 net.cpp:434] conv1_bn_scale <- conv1
I1027 18:42:52.912937 40125 net.cpp:395] conv1_bn_scale -> conv1 (in-place)
I1027 18:42:52.913005 40125 layer_factory.hpp:77] Creating layer conv1_bn_scale
I1027 18:42:52.913152 40125 net.cpp:150] Setting up conv1_bn_scale
I1027 18:42:52.913185 40125 net.cpp:157] Top shape: 128 12 32 32 (1572864)
I1027 18:42:52.913230 40125 net.cpp:165] Memory required for data: 25167872
I1027 18:42:52.913249 40125 layer_factory.hpp:77] Creating layer relu1
I1027 18:42:52.913278 40125 net.cpp:100] Creating Layer relu1
I1027 18:42:52.913291 40125 net.cpp:434] relu1 <- conv1
I1027 18:42:52.913306 40125 net.cpp:395] relu1 -> conv1 (in-place)
I1027 18:42:52.913336 40125 net.cpp:150] Setting up relu1
I1027 18:42:52.913353 40125 net.cpp:157] Top shape: 128 12 32 32 (1572864)
I1027 18:42:52.913391 40125 net.cpp:165] Memory required for data: 31459328
I1027 18:42:52.913408 40125 layer_factory.hpp:77] Creating layer conv2
I1027 18:42:52.913436 40125 net.cpp:100] Creating Layer conv2
I1027 18:42:52.913462 40125 net.cpp:434] conv2 <- conv1
I1027 18:42:52.913498 40125 net.cpp:408] conv2 -> conv2
I1027 18:42:52.919418 40125 net.cpp:150] Setting up conv2
I1027 18:42:52.919472 40125 net.cpp:157] Top shape: 128 20 16 16 (655360)
I1027 18:42:52.919504 40125 net.cpp:165] Memory required for data: 34080768
I1027 18:42:52.919524 40125 layer_factory.hpp:77] Creating layer conv2_bn
I1027 18:42:52.919574 40125 net.cpp:100] Creating Layer conv2_bn
I1027 18:42:52.919594 40125 net.cpp:434] conv2_bn <- conv2
I1027 18:42:52.919611 40125 net.cpp:395] conv2_bn -> conv2 (in-place)
I1027 18:42:52.919791 40125 net.cpp:150] Setting up conv2_bn
I1027 18:42:52.919818 40125 net.cpp:157] Top shape: 128 20 16 16 (655360)
I1027 18:42:52.919831 40125 net.cpp:165] Memory required for data: 36702208
I1027 18:42:52.919862 40125 layer_factory.hpp:77] Creating layer conv2_bn_scale
I1027 18:42:52.919883 40125 net.cpp:100] Creating Layer conv2_bn_scale
I1027 18:42:52.919934 40125 net.cpp:434] conv2_bn_scale <- conv2
I1027 18:42:52.919953 40125 net.cpp:395] conv2_bn_scale -> conv2 (in-place)
I1027 18:42:52.920004 40125 layer_factory.hpp:77] Creating layer conv2_bn_scale
I1027 18:42:52.920130 40125 net.cpp:150] Setting up conv2_bn_scale
I1027 18:42:52.920158 40125 net.cpp:157] Top shape: 128 20 16 16 (655360)
I1027 18:42:52.920171 40125 net.cpp:165] Memory required for data: 39323648
I1027 18:42:52.920192 40125 layer_factory.hpp:77] Creating layer relu2
I1027 18:42:52.920219 40125 net.cpp:100] Creating Layer relu2
I1027 18:42:52.920233 40125 net.cpp:434] relu2 <- conv2
I1027 18:42:52.920250 40125 net.cpp:395] relu2 -> conv2 (in-place)
I1027 18:42:52.920276 40125 net.cpp:150] Setting up relu2
I1027 18:42:52.920300 40125 net.cpp:157] Top shape: 128 20 16 16 (655360)
I1027 18:42:52.920313 40125 net.cpp:165] Memory required for data: 41945088
I1027 18:42:52.920325 40125 layer_factory.hpp:77] Creating layer conv3
I1027 18:42:52.920354 40125 net.cpp:100] Creating Layer conv3
I1027 18:42:52.920398 40125 net.cpp:434] conv3 <- conv2
I1027 18:42:52.920446 40125 net.cpp:408] conv3 -> conv3
I1027 18:42:52.924146 40125 net.cpp:150] Setting up conv3
I1027 18:42:52.924203 40125 net.cpp:157] Top shape: 128 40 8 8 (327680)
I1027 18:42:52.924218 40125 net.cpp:165] Memory required for data: 43255808
I1027 18:42:52.924238 40125 layer_factory.hpp:77] Creating layer conv3_bn
I1027 18:42:52.924268 40125 net.cpp:100] Creating Layer conv3_bn
I1027 18:42:52.924284 40125 net.cpp:434] conv3_bn <- conv3
I1027 18:42:52.924309 40125 net.cpp:395] conv3_bn -> conv3 (in-place)
I1027 18:42:52.924489 40125 net.cpp:150] Setting up conv3_bn
I1027 18:42:52.924513 40125 net.cpp:157] Top shape: 128 40 8 8 (327680)
I1027 18:42:52.924576 40125 net.cpp:165] Memory required for data: 44566528
I1027 18:42:52.924593 40125 layer_factory.hpp:77] Creating layer conv3_bn_scale
I1027 18:42:52.924616 40125 net.cpp:100] Creating Layer conv3_bn_scale
I1027 18:42:52.924636 40125 net.cpp:434] conv3_bn_scale <- conv3
I1027 18:42:52.924652 40125 net.cpp:395] conv3_bn_scale -> conv3 (in-place)
I1027 18:42:52.924703 40125 layer_factory.hpp:77] Creating layer conv3_bn_scale
I1027 18:42:52.924824 40125 net.cpp:150] Setting up conv3_bn_scale
I1027 18:42:52.924849 40125 net.cpp:157] Top shape: 128 40 8 8 (327680)
I1027 18:42:52.924861 40125 net.cpp:165] Memory required for data: 45877248
I1027 18:42:52.924888 40125 layer_factory.hpp:77] Creating layer relu3
I1027 18:42:52.924916 40125 net.cpp:100] Creating Layer relu3
I1027 18:42:52.924927 40125 net.cpp:434] relu3 <- conv3
I1027 18:42:52.924945 40125 net.cpp:395] relu3 -> conv3 (in-place)
I1027 18:42:52.924964 40125 net.cpp:150] Setting up relu3
I1027 18:42:52.924978 40125 net.cpp:157] Top shape: 128 40 8 8 (327680)
I1027 18:42:52.924989 40125 net.cpp:165] Memory required for data: 47187968
I1027 18:42:52.925009 40125 layer_factory.hpp:77] Creating layer conv4
I1027 18:42:52.925029 40125 net.cpp:100] Creating Layer conv4
I1027 18:42:52.925093 40125 net.cpp:434] conv4 <- conv3
I1027 18:42:52.925135 40125 net.cpp:408] conv4 -> conv4
I1027 18:42:52.928200 40125 net.cpp:150] Setting up conv4
I1027 18:42:52.928249 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.928290 40125 net.cpp:165] Memory required for data: 47712256
I1027 18:42:52.928339 40125 layer_factory.hpp:77] Creating layer conv4_bn
I1027 18:42:52.928380 40125 net.cpp:100] Creating Layer conv4_bn
I1027 18:42:52.928406 40125 net.cpp:434] conv4_bn <- conv4
I1027 18:42:52.928426 40125 net.cpp:395] conv4_bn -> conv4 (in-place)
I1027 18:42:52.928606 40125 net.cpp:150] Setting up conv4_bn
I1027 18:42:52.928644 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.928659 40125 net.cpp:165] Memory required for data: 48236544
I1027 18:42:52.928678 40125 layer_factory.hpp:77] Creating layer conv4_bn_scale
I1027 18:42:52.928699 40125 net.cpp:100] Creating Layer conv4_bn_scale
I1027 18:42:52.928719 40125 net.cpp:434] conv4_bn_scale <- conv4
I1027 18:42:52.928735 40125 net.cpp:395] conv4_bn_scale -> conv4 (in-place)
I1027 18:42:52.928784 40125 layer_factory.hpp:77] Creating layer conv4_bn_scale
I1027 18:42:52.928903 40125 net.cpp:150] Setting up conv4_bn_scale
I1027 18:42:52.928928 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.928941 40125 net.cpp:165] Memory required for data: 48760832
I1027 18:42:52.928956 40125 layer_factory.hpp:77] Creating layer relu4
I1027 18:42:52.928994 40125 net.cpp:100] Creating Layer relu4
I1027 18:42:52.929008 40125 net.cpp:434] relu4 <- conv4
I1027 18:42:52.929033 40125 net.cpp:395] relu4 -> conv4 (in-place)
I1027 18:42:52.929049 40125 net.cpp:150] Setting up relu4
I1027 18:42:52.929062 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.929075 40125 net.cpp:165] Memory required for data: 49285120
I1027 18:42:52.929095 40125 layer_factory.hpp:77] Creating layer conv5
I1027 18:42:52.929136 40125 net.cpp:100] Creating Layer conv5
I1027 18:42:52.929178 40125 net.cpp:434] conv5 <- conv4
I1027 18:42:52.929195 40125 net.cpp:408] conv5 -> conv5
I1027 18:42:52.929603 40125 net.cpp:150] Setting up conv5
I1027 18:42:52.929662 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.929674 40125 net.cpp:165] Memory required for data: 49809408
I1027 18:42:52.929705 40125 layer_factory.hpp:77] Creating layer conv5_bn
I1027 18:42:52.929800 40125 net.cpp:100] Creating Layer conv5_bn
I1027 18:42:52.929836 40125 net.cpp:434] conv5_bn <- conv5
I1027 18:42:52.929850 40125 net.cpp:395] conv5_bn -> conv5 (in-place)
I1027 18:42:52.930075 40125 net.cpp:150] Setting up conv5_bn
I1027 18:42:52.930109 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.930122 40125 net.cpp:165] Memory required for data: 50333696
I1027 18:42:52.930137 40125 layer_factory.hpp:77] Creating layer conv5_bn_scale
I1027 18:42:52.930531 40125 net.cpp:100] Creating Layer conv5_bn_scale
I1027 18:42:52.930546 40125 net.cpp:434] conv5_bn_scale <- conv5
I1027 18:42:52.930591 40125 net.cpp:395] conv5_bn_scale -> conv5 (in-place)
I1027 18:42:52.930644 40125 layer_factory.hpp:77] Creating layer conv5_bn_scale
I1027 18:42:52.930764 40125 net.cpp:150] Setting up conv5_bn_scale
I1027 18:42:52.930790 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.930804 40125 net.cpp:165] Memory required for data: 50857984
I1027 18:42:52.930817 40125 layer_factory.hpp:77] Creating layer relu5
I1027 18:42:52.930835 40125 net.cpp:100] Creating Layer relu5
I1027 18:42:52.930846 40125 net.cpp:434] relu5 <- conv5
I1027 18:42:52.930863 40125 net.cpp:395] relu5 -> conv5 (in-place)
I1027 18:42:52.930886 40125 net.cpp:150] Setting up relu5
I1027 18:42:52.930900 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.930924 40125 net.cpp:165] Memory required for data: 51382272
I1027 18:42:52.930949 40125 layer_factory.hpp:77] Creating layer ip1
I1027 18:42:52.930969 40125 net.cpp:100] Creating Layer ip1
I1027 18:42:52.930982 40125 net.cpp:434] ip1 <- conv5
I1027 18:42:52.931010 40125 net.cpp:408] ip1 -> ip1
I1027 18:42:52.934830 40125 net.cpp:150] Setting up ip1
I1027 18:42:52.934886 40125 net.cpp:157] Top shape: 128 128 (16384)
I1027 18:42:52.934900 40125 net.cpp:165] Memory required for data: 51447808
I1027 18:42:52.934923 40125 layer_factory.hpp:77] Creating layer ip1_bn
I1027 18:42:52.934991 40125 net.cpp:100] Creating Layer ip1_bn
I1027 18:42:52.935034 40125 net.cpp:434] ip1_bn <- ip1
I1027 18:42:52.935092 40125 net.cpp:395] ip1_bn -> ip1 (in-place)
I1027 18:42:52.935259 40125 net.cpp:150] Setting up ip1_bn
I1027 18:42:52.935284 40125 net.cpp:157] Top shape: 128 128 (16384)
I1027 18:42:52.935297 40125 net.cpp:165] Memory required for data: 51513344
I1027 18:42:52.935324 40125 layer_factory.hpp:77] Creating layer ip1_bn_scale
I1027 18:42:52.935349 40125 net.cpp:100] Creating Layer ip1_bn_scale
I1027 18:42:52.935362 40125 net.cpp:434] ip1_bn_scale <- ip1
I1027 18:42:52.935384 40125 net.cpp:395] ip1_bn_scale -> ip1 (in-place)
I1027 18:42:52.935477 40125 layer_factory.hpp:77] Creating layer ip1_bn_scale
I1027 18:42:52.935623 40125 net.cpp:150] Setting up ip1_bn_scale
I1027 18:42:52.935653 40125 net.cpp:157] Top shape: 128 128 (16384)
I1027 18:42:52.935667 40125 net.cpp:165] Memory required for data: 51578880
I1027 18:42:52.935683 40125 layer_factory.hpp:77] Creating layer relu-ip1
I1027 18:42:52.935715 40125 net.cpp:100] Creating Layer relu-ip1
I1027 18:42:52.935729 40125 net.cpp:434] relu-ip1 <- ip1
I1027 18:42:52.935763 40125 net.cpp:395] relu-ip1 -> ip1 (in-place)
I1027 18:42:52.935789 40125 net.cpp:150] Setting up relu-ip1
I1027 18:42:52.935802 40125 net.cpp:157] Top shape: 128 128 (16384)
I1027 18:42:52.935847 40125 net.cpp:165] Memory required for data: 51644416
I1027 18:42:52.935865 40125 layer_factory.hpp:77] Creating layer fullyconnected1
I1027 18:42:52.935884 40125 net.cpp:100] Creating Layer fullyconnected1
I1027 18:42:52.935911 40125 net.cpp:434] fullyconnected1 <- ip1
I1027 18:42:52.935948 40125 net.cpp:408] fullyconnected1 -> fullyconnected1
I1027 18:42:52.936089 40125 net.cpp:150] Setting up fullyconnected1
I1027 18:42:52.936120 40125 net.cpp:157] Top shape: 128 4 (512)
I1027 18:42:52.936131 40125 net.cpp:165] Memory required for data: 51646464
I1027 18:42:52.936146 40125 layer_factory.hpp:77] Creating layer smallRegressionLoss
I1027 18:42:52.936162 40125 net.cpp:100] Creating Layer smallRegressionLoss
I1027 18:42:52.936174 40125 net.cpp:434] smallRegressionLoss <- fullyconnected1
I1027 18:42:52.936187 40125 net.cpp:434] smallRegressionLoss <- roi
I1027 18:42:52.936221 40125 net.cpp:408] smallRegressionLoss -> RegressionLoss
I1027 18:42:52.936305 40125 net.cpp:150] Setting up smallRegressionLoss
I1027 18:42:52.936324 40125 net.cpp:157] Top shape: (1)
I1027 18:42:52.936336 40125 net.cpp:160]     with loss weight 1
I1027 18:42:52.936410 40125 net.cpp:165] Memory required for data: 51646468
I1027 18:42:52.936486 40125 net.cpp:226] smallRegressionLoss needs backward computation.
I1027 18:42:52.936522 40125 net.cpp:226] fullyconnected1 needs backward computation.
I1027 18:42:52.936539 40125 net.cpp:226] relu-ip1 needs backward computation.
I1027 18:42:52.936581 40125 net.cpp:226] ip1_bn_scale needs backward computation.
I1027 18:42:52.936594 40125 net.cpp:226] ip1_bn needs backward computation.
I1027 18:42:52.936645 40125 net.cpp:226] ip1 needs backward computation.
I1027 18:42:52.936657 40125 net.cpp:226] relu5 needs backward computation.
I1027 18:42:52.936672 40125 net.cpp:226] conv5_bn_scale needs backward computation.
I1027 18:42:52.936687 40125 net.cpp:226] conv5_bn needs backward computation.
I1027 18:42:52.936703 40125 net.cpp:226] conv5 needs backward computation.
I1027 18:42:52.936717 40125 net.cpp:226] relu4 needs backward computation.
I1027 18:42:52.936738 40125 net.cpp:226] conv4_bn_scale needs backward computation.
I1027 18:42:52.936794 40125 net.cpp:226] conv4_bn needs backward computation.
I1027 18:42:52.936822 40125 net.cpp:226] conv4 needs backward computation.
I1027 18:42:52.936851 40125 net.cpp:226] relu3 needs backward computation.
I1027 18:42:52.936872 40125 net.cpp:226] conv3_bn_scale needs backward computation.
I1027 18:42:52.936893 40125 net.cpp:226] conv3_bn needs backward computation.
I1027 18:42:52.936913 40125 net.cpp:226] conv3 needs backward computation.
I1027 18:42:52.936926 40125 net.cpp:226] relu2 needs backward computation.
I1027 18:42:52.936939 40125 net.cpp:226] conv2_bn_scale needs backward computation.
I1027 18:42:52.936975 40125 net.cpp:226] conv2_bn needs backward computation.
I1027 18:42:52.936995 40125 net.cpp:226] conv2 needs backward computation.
I1027 18:42:52.937011 40125 net.cpp:226] relu1 needs backward computation.
I1027 18:42:52.937023 40125 net.cpp:226] conv1_bn_scale needs backward computation.
I1027 18:42:52.937038 40125 net.cpp:226] conv1_bn needs backward computation.
I1027 18:42:52.937054 40125 net.cpp:226] conv1 needs backward computation.
I1027 18:42:52.937070 40125 net.cpp:228] input-data does not need backward computation.
I1027 18:42:52.937088 40125 net.cpp:270] This network produces output RegressionLoss
I1027 18:42:52.937119 40125 net.cpp:283] Network initialization done.
I1027 18:42:52.937628 40125 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/hand_reg/alimouth/traintest.prototxt
I1027 18:42:52.937667 40125 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1027 18:42:52.937690 40125 solver.cpp:196] Creating test net (#0) specified by net file: examples/hand_reg/alimouth/traintest.prototxt
I1027 18:42:52.937739 40125 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer input-data
I1027 18:42:52.937979 40125 net.cpp:58] Initializing net from parameters: 
name: "hand_gesture_reg_mouth64"
state {
  phase: TEST
}
layer {
  name: "test-data"
  type: "Python"
  top: "data"
  top: "roi"
  include {
    phase: TEST
  }
  python_param {
    module: "pythonLayer"
    layer: "Data_Layer_test"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "conv1_bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "conv1_bn_scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "conv2_bn"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "conv2_bn_scale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "conv2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "conv3_bn"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "conv3_bn_scale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "conv4_bn"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "conv4_bn_scale"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "conv5_bn"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "conv5_bn_scale"
  type: "Scale"
  bottom: "conv5"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "conv5"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  inner_product_param {
    num_output: 128
    bias_term: false
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "ip1_bn"
  type: "BatchNorm"
  bottom: "ip1"
  top: "ip1"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.9
    eps: 2e-05
  }
}
layer {
  name: "ip1_bn_scale"
  type: "Scale"
  bottom: "ip1"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "relu-ip1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "fullyconnected1"
  type: "InnerProduct"
  bottom: "ip1"
  top: "fullyconnected1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "smallRegressionLoss"
  type: "EuclideanLoss"
  bottom: "fullyconnected1"
  bottom: "roi"
  top: "RegressionLoss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
}
I1027 18:42:52.938150 40125 layer_factory.hpp:77] Creating layer test-data
I1027 18:42:52.938410 40125 net.cpp:100] Creating Layer test-data
I1027 18:42:52.938472 40125 net.cpp:408] test-data -> data
I1027 18:42:52.938547 40125 net.cpp:408] test-data -> roi
I1027 18:42:52.967442 40125 net.cpp:150] Setting up test-data
I1027 18:42:52.967530 40125 net.cpp:157] Top shape: 128 3 64 64 (1572864)
I1027 18:42:52.967546 40125 net.cpp:157] Top shape: 128 4 (512)
I1027 18:42:52.967558 40125 net.cpp:165] Memory required for data: 6293504
I1027 18:42:52.967576 40125 layer_factory.hpp:77] Creating layer conv1
I1027 18:42:52.967617 40125 net.cpp:100] Creating Layer conv1
I1027 18:42:52.967633 40125 net.cpp:434] conv1 <- data
I1027 18:42:52.967655 40125 net.cpp:408] conv1 -> conv1
I1027 18:42:52.967862 40125 net.cpp:150] Setting up conv1
I1027 18:42:52.967895 40125 net.cpp:157] Top shape: 128 12 32 32 (1572864)
I1027 18:42:52.967907 40125 net.cpp:165] Memory required for data: 12584960
I1027 18:42:52.967927 40125 layer_factory.hpp:77] Creating layer conv1_bn
I1027 18:42:52.967964 40125 net.cpp:100] Creating Layer conv1_bn
I1027 18:42:52.967979 40125 net.cpp:434] conv1_bn <- conv1
I1027 18:42:52.967995 40125 net.cpp:395] conv1_bn -> conv1 (in-place)
I1027 18:42:52.968200 40125 net.cpp:150] Setting up conv1_bn
I1027 18:42:52.968228 40125 net.cpp:157] Top shape: 128 12 32 32 (1572864)
I1027 18:42:52.968258 40125 net.cpp:165] Memory required for data: 18876416
I1027 18:42:52.968287 40125 layer_factory.hpp:77] Creating layer conv1_bn_scale
I1027 18:42:52.968315 40125 net.cpp:100] Creating Layer conv1_bn_scale
I1027 18:42:52.968333 40125 net.cpp:434] conv1_bn_scale <- conv1
I1027 18:42:52.968355 40125 net.cpp:395] conv1_bn_scale -> conv1 (in-place)
I1027 18:42:52.968410 40125 layer_factory.hpp:77] Creating layer conv1_bn_scale
I1027 18:42:52.968525 40125 net.cpp:150] Setting up conv1_bn_scale
I1027 18:42:52.968554 40125 net.cpp:157] Top shape: 128 12 32 32 (1572864)
I1027 18:42:52.968567 40125 net.cpp:165] Memory required for data: 25167872
I1027 18:42:52.968585 40125 layer_factory.hpp:77] Creating layer relu1
I1027 18:42:52.968608 40125 net.cpp:100] Creating Layer relu1
I1027 18:42:52.968624 40125 net.cpp:434] relu1 <- conv1
I1027 18:42:52.968641 40125 net.cpp:395] relu1 -> conv1 (in-place)
I1027 18:42:52.968662 40125 net.cpp:150] Setting up relu1
I1027 18:42:52.968677 40125 net.cpp:157] Top shape: 128 12 32 32 (1572864)
I1027 18:42:52.968693 40125 net.cpp:165] Memory required for data: 31459328
I1027 18:42:52.968705 40125 layer_factory.hpp:77] Creating layer conv2
I1027 18:42:52.968724 40125 net.cpp:100] Creating Layer conv2
I1027 18:42:52.968739 40125 net.cpp:434] conv2 <- conv1
I1027 18:42:52.968756 40125 net.cpp:408] conv2 -> conv2
I1027 18:42:52.968931 40125 net.cpp:150] Setting up conv2
I1027 18:42:52.968957 40125 net.cpp:157] Top shape: 128 20 16 16 (655360)
I1027 18:42:52.968971 40125 net.cpp:165] Memory required for data: 34080768
I1027 18:42:52.968986 40125 layer_factory.hpp:77] Creating layer conv2_bn
I1027 18:42:52.969005 40125 net.cpp:100] Creating Layer conv2_bn
I1027 18:42:52.969018 40125 net.cpp:434] conv2_bn <- conv2
I1027 18:42:52.969033 40125 net.cpp:395] conv2_bn -> conv2 (in-place)
I1027 18:42:52.969216 40125 net.cpp:150] Setting up conv2_bn
I1027 18:42:52.969245 40125 net.cpp:157] Top shape: 128 20 16 16 (655360)
I1027 18:42:52.969276 40125 net.cpp:165] Memory required for data: 36702208
I1027 18:42:52.969296 40125 layer_factory.hpp:77] Creating layer conv2_bn_scale
I1027 18:42:52.969318 40125 net.cpp:100] Creating Layer conv2_bn_scale
I1027 18:42:52.969378 40125 net.cpp:434] conv2_bn_scale <- conv2
I1027 18:42:52.969398 40125 net.cpp:395] conv2_bn_scale -> conv2 (in-place)
I1027 18:42:52.969445 40125 layer_factory.hpp:77] Creating layer conv2_bn_scale
I1027 18:42:52.969564 40125 net.cpp:150] Setting up conv2_bn_scale
I1027 18:42:52.969594 40125 net.cpp:157] Top shape: 128 20 16 16 (655360)
I1027 18:42:52.969606 40125 net.cpp:165] Memory required for data: 39323648
I1027 18:42:52.969625 40125 layer_factory.hpp:77] Creating layer relu2
I1027 18:42:52.969642 40125 net.cpp:100] Creating Layer relu2
I1027 18:42:52.969658 40125 net.cpp:434] relu2 <- conv2
I1027 18:42:52.969676 40125 net.cpp:395] relu2 -> conv2 (in-place)
I1027 18:42:52.969694 40125 net.cpp:150] Setting up relu2
I1027 18:42:52.969720 40125 net.cpp:157] Top shape: 128 20 16 16 (655360)
I1027 18:42:52.969738 40125 net.cpp:165] Memory required for data: 41945088
I1027 18:42:52.969749 40125 layer_factory.hpp:77] Creating layer conv3
I1027 18:42:52.969768 40125 net.cpp:100] Creating Layer conv3
I1027 18:42:52.969791 40125 net.cpp:434] conv3 <- conv2
I1027 18:42:52.969820 40125 net.cpp:408] conv3 -> conv3
I1027 18:42:52.970026 40125 net.cpp:150] Setting up conv3
I1027 18:42:52.970055 40125 net.cpp:157] Top shape: 128 40 8 8 (327680)
I1027 18:42:52.970067 40125 net.cpp:165] Memory required for data: 43255808
I1027 18:42:52.970082 40125 layer_factory.hpp:77] Creating layer conv3_bn
I1027 18:42:52.970111 40125 net.cpp:100] Creating Layer conv3_bn
I1027 18:42:52.970137 40125 net.cpp:434] conv3_bn <- conv3
I1027 18:42:52.970319 40125 net.cpp:395] conv3_bn -> conv3 (in-place)
I1027 18:42:52.970521 40125 net.cpp:150] Setting up conv3_bn
I1027 18:42:52.970567 40125 net.cpp:157] Top shape: 128 40 8 8 (327680)
I1027 18:42:52.970587 40125 net.cpp:165] Memory required for data: 44566528
I1027 18:42:52.970604 40125 layer_factory.hpp:77] Creating layer conv3_bn_scale
I1027 18:42:52.970624 40125 net.cpp:100] Creating Layer conv3_bn_scale
I1027 18:42:52.970657 40125 net.cpp:434] conv3_bn_scale <- conv3
I1027 18:42:52.970675 40125 net.cpp:395] conv3_bn_scale -> conv3 (in-place)
I1027 18:42:52.970724 40125 layer_factory.hpp:77] Creating layer conv3_bn_scale
I1027 18:42:52.970840 40125 net.cpp:150] Setting up conv3_bn_scale
I1027 18:42:52.970865 40125 net.cpp:157] Top shape: 128 40 8 8 (327680)
I1027 18:42:52.970878 40125 net.cpp:165] Memory required for data: 45877248
I1027 18:42:52.970898 40125 layer_factory.hpp:77] Creating layer relu3
I1027 18:42:52.970917 40125 net.cpp:100] Creating Layer relu3
I1027 18:42:52.970929 40125 net.cpp:434] relu3 <- conv3
I1027 18:42:52.970952 40125 net.cpp:395] relu3 -> conv3 (in-place)
I1027 18:42:52.970998 40125 net.cpp:150] Setting up relu3
I1027 18:42:52.971015 40125 net.cpp:157] Top shape: 128 40 8 8 (327680)
I1027 18:42:52.971026 40125 net.cpp:165] Memory required for data: 47187968
I1027 18:42:52.971040 40125 layer_factory.hpp:77] Creating layer conv4
I1027 18:42:52.971062 40125 net.cpp:100] Creating Layer conv4
I1027 18:42:52.971078 40125 net.cpp:434] conv4 <- conv3
I1027 18:42:52.971104 40125 net.cpp:408] conv4 -> conv4
I1027 18:42:52.971401 40125 net.cpp:150] Setting up conv4
I1027 18:42:52.971431 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.971444 40125 net.cpp:165] Memory required for data: 47712256
I1027 18:42:52.971458 40125 layer_factory.hpp:77] Creating layer conv4_bn
I1027 18:42:52.971477 40125 net.cpp:100] Creating Layer conv4_bn
I1027 18:42:52.971489 40125 net.cpp:434] conv4_bn <- conv4
I1027 18:42:52.971508 40125 net.cpp:395] conv4_bn -> conv4 (in-place)
I1027 18:42:52.971673 40125 net.cpp:150] Setting up conv4_bn
I1027 18:42:52.971700 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.971724 40125 net.cpp:165] Memory required for data: 48236544
I1027 18:42:52.971747 40125 layer_factory.hpp:77] Creating layer conv4_bn_scale
I1027 18:42:52.971765 40125 net.cpp:100] Creating Layer conv4_bn_scale
I1027 18:42:52.971777 40125 net.cpp:434] conv4_bn_scale <- conv4
I1027 18:42:52.971796 40125 net.cpp:395] conv4_bn_scale -> conv4 (in-place)
I1027 18:42:52.971876 40125 layer_factory.hpp:77] Creating layer conv4_bn_scale
I1027 18:42:52.971993 40125 net.cpp:150] Setting up conv4_bn_scale
I1027 18:42:52.972020 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.972033 40125 net.cpp:165] Memory required for data: 48760832
I1027 18:42:52.972054 40125 layer_factory.hpp:77] Creating layer relu4
I1027 18:42:52.972074 40125 net.cpp:100] Creating Layer relu4
I1027 18:42:52.972110 40125 net.cpp:434] relu4 <- conv4
I1027 18:42:52.972139 40125 net.cpp:395] relu4 -> conv4 (in-place)
I1027 18:42:52.972157 40125 net.cpp:150] Setting up relu4
I1027 18:42:52.972170 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.972184 40125 net.cpp:165] Memory required for data: 49285120
I1027 18:42:52.972196 40125 layer_factory.hpp:77] Creating layer conv5
I1027 18:42:52.972216 40125 net.cpp:100] Creating Layer conv5
I1027 18:42:52.972231 40125 net.cpp:434] conv5 <- conv4
I1027 18:42:52.972247 40125 net.cpp:408] conv5 -> conv5
I1027 18:42:52.972621 40125 net.cpp:150] Setting up conv5
I1027 18:42:52.972652 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.972676 40125 net.cpp:165] Memory required for data: 49809408
I1027 18:42:52.972698 40125 layer_factory.hpp:77] Creating layer conv5_bn
I1027 18:42:52.972715 40125 net.cpp:100] Creating Layer conv5_bn
I1027 18:42:52.972733 40125 net.cpp:434] conv5_bn <- conv5
I1027 18:42:52.972748 40125 net.cpp:395] conv5_bn -> conv5 (in-place)
I1027 18:42:52.972919 40125 net.cpp:150] Setting up conv5_bn
I1027 18:42:52.972946 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.972965 40125 net.cpp:165] Memory required for data: 50333696
I1027 18:42:52.972987 40125 layer_factory.hpp:77] Creating layer conv5_bn_scale
I1027 18:42:52.973007 40125 net.cpp:100] Creating Layer conv5_bn_scale
I1027 18:42:52.973032 40125 net.cpp:434] conv5_bn_scale <- conv5
I1027 18:42:52.973054 40125 net.cpp:395] conv5_bn_scale -> conv5 (in-place)
I1027 18:42:52.973137 40125 layer_factory.hpp:77] Creating layer conv5_bn_scale
I1027 18:42:52.973260 40125 net.cpp:150] Setting up conv5_bn_scale
I1027 18:42:52.973301 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.973320 40125 net.cpp:165] Memory required for data: 50857984
I1027 18:42:52.973351 40125 layer_factory.hpp:77] Creating layer relu5
I1027 18:42:52.973388 40125 net.cpp:100] Creating Layer relu5
I1027 18:42:52.973400 40125 net.cpp:434] relu5 <- conv5
I1027 18:42:52.973428 40125 net.cpp:395] relu5 -> conv5 (in-place)
I1027 18:42:52.973449 40125 net.cpp:150] Setting up relu5
I1027 18:42:52.973472 40125 net.cpp:157] Top shape: 128 64 4 4 (131072)
I1027 18:42:52.973484 40125 net.cpp:165] Memory required for data: 51382272
I1027 18:42:52.973496 40125 layer_factory.hpp:77] Creating layer ip1
I1027 18:42:52.973517 40125 net.cpp:100] Creating Layer ip1
I1027 18:42:52.973538 40125 net.cpp:434] ip1 <- conv5
I1027 18:42:52.973573 40125 net.cpp:408] ip1 -> ip1
I1027 18:42:52.974572 40125 net.cpp:150] Setting up ip1
I1027 18:42:52.974606 40125 net.cpp:157] Top shape: 128 128 (16384)
I1027 18:42:52.974619 40125 net.cpp:165] Memory required for data: 51447808
I1027 18:42:52.974689 40125 layer_factory.hpp:77] Creating layer ip1_bn
I1027 18:42:52.974720 40125 net.cpp:100] Creating Layer ip1_bn
I1027 18:42:52.974732 40125 net.cpp:434] ip1_bn <- ip1
I1027 18:42:52.974748 40125 net.cpp:395] ip1_bn -> ip1 (in-place)
I1027 18:42:52.974903 40125 net.cpp:150] Setting up ip1_bn
I1027 18:42:52.974930 40125 net.cpp:157] Top shape: 128 128 (16384)
I1027 18:42:52.974947 40125 net.cpp:165] Memory required for data: 51513344
I1027 18:42:52.974970 40125 layer_factory.hpp:77] Creating layer ip1_bn_scale
I1027 18:42:52.974992 40125 net.cpp:100] Creating Layer ip1_bn_scale
I1027 18:42:52.975004 40125 net.cpp:434] ip1_bn_scale <- ip1
I1027 18:42:52.975020 40125 net.cpp:395] ip1_bn_scale -> ip1 (in-place)
I1027 18:42:52.975070 40125 layer_factory.hpp:77] Creating layer ip1_bn_scale
I1027 18:42:52.975186 40125 net.cpp:150] Setting up ip1_bn_scale
I1027 18:42:52.975212 40125 net.cpp:157] Top shape: 128 128 (16384)
I1027 18:42:52.975267 40125 net.cpp:165] Memory required for data: 51578880
I1027 18:42:52.975284 40125 layer_factory.hpp:77] Creating layer relu-ip1
I1027 18:42:52.975301 40125 net.cpp:100] Creating Layer relu-ip1
I1027 18:42:52.975324 40125 net.cpp:434] relu-ip1 <- ip1
I1027 18:42:52.975344 40125 net.cpp:395] relu-ip1 -> ip1 (in-place)
I1027 18:42:52.975359 40125 net.cpp:150] Setting up relu-ip1
I1027 18:42:52.975374 40125 net.cpp:157] Top shape: 128 128 (16384)
I1027 18:42:52.975389 40125 net.cpp:165] Memory required for data: 51644416
I1027 18:42:52.975400 40125 layer_factory.hpp:77] Creating layer fullyconnected1
I1027 18:42:52.975428 40125 net.cpp:100] Creating Layer fullyconnected1
I1027 18:42:52.975450 40125 net.cpp:434] fullyconnected1 <- ip1
I1027 18:42:52.975472 40125 net.cpp:408] fullyconnected1 -> fullyconnected1
I1027 18:42:52.975579 40125 net.cpp:150] Setting up fullyconnected1
I1027 18:42:52.975612 40125 net.cpp:157] Top shape: 128 4 (512)
I1027 18:42:52.975625 40125 net.cpp:165] Memory required for data: 51646464
I1027 18:42:52.975639 40125 layer_factory.hpp:77] Creating layer smallRegressionLoss
I1027 18:42:52.975687 40125 net.cpp:100] Creating Layer smallRegressionLoss
I1027 18:42:52.975708 40125 net.cpp:434] smallRegressionLoss <- fullyconnected1
I1027 18:42:52.975740 40125 net.cpp:434] smallRegressionLoss <- roi
I1027 18:42:52.975775 40125 net.cpp:408] smallRegressionLoss -> RegressionLoss
I1027 18:42:52.975822 40125 net.cpp:150] Setting up smallRegressionLoss
I1027 18:42:52.975838 40125 net.cpp:157] Top shape: (1)
I1027 18:42:52.975853 40125 net.cpp:160]     with loss weight 1
I1027 18:42:52.975883 40125 net.cpp:165] Memory required for data: 51646468
I1027 18:42:52.975896 40125 net.cpp:226] smallRegressionLoss needs backward computation.
I1027 18:42:52.975909 40125 net.cpp:226] fullyconnected1 needs backward computation.
I1027 18:42:52.975924 40125 net.cpp:226] relu-ip1 needs backward computation.
I1027 18:42:52.975937 40125 net.cpp:226] ip1_bn_scale needs backward computation.
I1027 18:42:52.975949 40125 net.cpp:226] ip1_bn needs backward computation.
I1027 18:42:52.975960 40125 net.cpp:226] ip1 needs backward computation.
I1027 18:42:52.975970 40125 net.cpp:226] relu5 needs backward computation.
I1027 18:42:52.975984 40125 net.cpp:226] conv5_bn_scale needs backward computation.
I1027 18:42:52.975996 40125 net.cpp:226] conv5_bn needs backward computation.
I1027 18:42:52.976007 40125 net.cpp:226] conv5 needs backward computation.
I1027 18:42:52.976043 40125 net.cpp:226] relu4 needs backward computation.
I1027 18:42:52.976056 40125 net.cpp:226] conv4_bn_scale needs backward computation.
I1027 18:42:52.976068 40125 net.cpp:226] conv4_bn needs backward computation.
I1027 18:42:52.976104 40125 net.cpp:226] conv4 needs backward computation.
I1027 18:42:52.976131 40125 net.cpp:226] relu3 needs backward computation.
I1027 18:42:52.976155 40125 net.cpp:226] conv3_bn_scale needs backward computation.
I1027 18:42:52.976167 40125 net.cpp:226] conv3_bn needs backward computation.
I1027 18:42:52.976192 40125 net.cpp:226] conv3 needs backward computation.
I1027 18:42:52.976205 40125 net.cpp:226] relu2 needs backward computation.
I1027 18:42:52.976217 40125 net.cpp:226] conv2_bn_scale needs backward computation.
I1027 18:42:52.976230 40125 net.cpp:226] conv2_bn needs backward computation.
I1027 18:42:52.976243 40125 net.cpp:226] conv2 needs backward computation.
I1027 18:42:52.976258 40125 net.cpp:226] relu1 needs backward computation.
I1027 18:42:52.976270 40125 net.cpp:226] conv1_bn_scale needs backward computation.
I1027 18:42:52.976281 40125 net.cpp:226] conv1_bn needs backward computation.
I1027 18:42:52.976292 40125 net.cpp:226] conv1 needs backward computation.
I1027 18:42:52.976307 40125 net.cpp:228] test-data does not need backward computation.
I1027 18:42:52.976320 40125 net.cpp:270] This network produces output RegressionLoss
I1027 18:42:52.976346 40125 net.cpp:283] Network initialization done.
I1027 18:42:52.976465 40125 solver.cpp:75] Solver scaffolding done.
I1027 18:42:52.977593 40125 caffe.cpp:155] Finetuning from models/mouth/1026f1025_iter_1500000.caffemodel
I1027 18:42:52.980192 40125 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: models/mouth/1026f1025_iter_1500000.caffemodel
I1027 18:42:52.980264 40125 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1027 18:42:52.981494 40125 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: models/mouth/1026f1025_iter_1500000.caffemodel
I1027 18:42:52.981608 40125 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1027 18:42:52.981645 40125 net.cpp:761] Ignoring source layer input-data
I1027 18:42:52.981992 40125 caffe.cpp:251] Starting Optimization
I1027 18:42:52.982074 40125 solver.cpp:294] Solving hand_gesture_reg_mouth64
I1027 18:42:52.982116 40125 solver.cpp:295] Learning Rate Policy: step
I1027 18:42:52.992566 40125 solver.cpp:358] Iteration 0, Testing net (#0)
I1027 18:42:52.992655 40125 net.cpp:693] Ignoring source layer input-data
I1027 18:42:57.597626 40125 solver.cpp:425]     Test net output #0: RegressionLoss = 0.343766 (* 1 = 0.343766 loss)
I1027 18:43:01.352856 40125 solver.cpp:243] Iteration 0, loss = 0.389932
I1027 18:43:01.353713 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.389932 (* 1 = 0.389932 loss)
I1027 18:43:01.353834 40125 sgd_solver.cpp:138] Iteration 0, lr = 0.0005
I1027 18:46:21.495697 40125 solver.cpp:243] Iteration 50, loss = 0.739145
I1027 18:46:21.495890 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.739145 (* 1 = 0.739145 loss)
I1027 18:46:21.495967 40125 sgd_solver.cpp:138] Iteration 50, lr = 0.0005
I1027 18:49:44.350824 40125 solver.cpp:243] Iteration 100, loss = 0.748098
I1027 18:49:44.351722 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.748098 (* 1 = 0.748098 loss)
I1027 18:49:44.351748 40125 sgd_solver.cpp:138] Iteration 100, lr = 0.0005
I1027 18:53:11.450129 40125 solver.cpp:243] Iteration 150, loss = 0.597245
I1027 18:53:11.450814 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.597245 (* 1 = 0.597245 loss)
I1027 18:53:11.450841 40125 sgd_solver.cpp:138] Iteration 150, lr = 0.0005
I1027 18:56:28.400112 40125 solver.cpp:243] Iteration 200, loss = 0.567783
I1027 18:56:28.400461 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.567783 (* 1 = 0.567783 loss)
I1027 18:56:28.400496 40125 sgd_solver.cpp:138] Iteration 200, lr = 0.0005
I1027 18:59:49.917903 40125 solver.cpp:243] Iteration 250, loss = 0.730261
I1027 18:59:49.919111 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.730261 (* 1 = 0.730261 loss)
I1027 18:59:49.919138 40125 sgd_solver.cpp:138] Iteration 250, lr = 0.0005
I1027 19:03:22.604348 40125 solver.cpp:243] Iteration 300, loss = 0.572567
I1027 19:03:22.604878 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.572567 (* 1 = 0.572567 loss)
I1027 19:03:22.604915 40125 sgd_solver.cpp:138] Iteration 300, lr = 0.0005
I1027 19:06:48.072250 40125 solver.cpp:243] Iteration 350, loss = 0.842547
I1027 19:06:48.072654 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.842547 (* 1 = 0.842547 loss)
I1027 19:06:48.072687 40125 sgd_solver.cpp:138] Iteration 350, lr = 0.0005
I1027 19:10:00.556468 40125 solver.cpp:243] Iteration 400, loss = 0.728357
I1027 19:10:00.557068 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.728357 (* 1 = 0.728357 loss)
I1027 19:10:00.557132 40125 sgd_solver.cpp:138] Iteration 400, lr = 0.0005
I1027 19:13:25.779994 40125 solver.cpp:243] Iteration 450, loss = 0.502301
I1027 19:13:25.781906 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.502301 (* 1 = 0.502301 loss)
I1027 19:13:25.781970 40125 sgd_solver.cpp:138] Iteration 450, lr = 0.0005
I1027 19:16:49.635665 40125 solver.cpp:243] Iteration 500, loss = 0.634683
I1027 19:16:49.636560 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.634683 (* 1 = 0.634683 loss)
I1027 19:16:49.636607 40125 sgd_solver.cpp:138] Iteration 500, lr = 0.0005
I1027 19:20:06.621982 40125 solver.cpp:243] Iteration 550, loss = 0.448058
I1027 19:20:06.622843 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.448058 (* 1 = 0.448058 loss)
I1027 19:20:06.622871 40125 sgd_solver.cpp:138] Iteration 550, lr = 0.0005
I1027 19:23:29.947698 40125 solver.cpp:243] Iteration 600, loss = 0.574612
I1027 19:23:29.948082 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.574612 (* 1 = 0.574612 loss)
I1027 19:23:29.948118 40125 sgd_solver.cpp:138] Iteration 600, lr = 0.0005
I1027 19:26:44.854459 40125 solver.cpp:243] Iteration 650, loss = 0.47324
I1027 19:26:44.854863 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.47324 (* 1 = 0.47324 loss)
I1027 19:26:44.854892 40125 sgd_solver.cpp:138] Iteration 650, lr = 0.0005
I1027 19:29:54.768898 40125 solver.cpp:243] Iteration 700, loss = 0.667062
I1027 19:29:54.771363 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.667062 (* 1 = 0.667062 loss)
I1027 19:29:54.771438 40125 sgd_solver.cpp:138] Iteration 700, lr = 0.0005
I1027 19:33:08.768445 40125 solver.cpp:243] Iteration 750, loss = 0.51096
I1027 19:33:08.768859 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.51096 (* 1 = 0.51096 loss)
I1027 19:33:08.768890 40125 sgd_solver.cpp:138] Iteration 750, lr = 0.0005
I1027 19:36:16.337225 40125 solver.cpp:243] Iteration 800, loss = 0.527306
I1027 19:36:16.337905 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.527306 (* 1 = 0.527306 loss)
I1027 19:36:16.337975 40125 sgd_solver.cpp:138] Iteration 800, lr = 0.0005
I1027 19:39:34.927449 40125 solver.cpp:243] Iteration 850, loss = 0.438542
I1027 19:39:34.927901 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.438542 (* 1 = 0.438542 loss)
I1027 19:39:34.927933 40125 sgd_solver.cpp:138] Iteration 850, lr = 0.0005
I1027 19:43:00.399027 40125 solver.cpp:243] Iteration 900, loss = 0.476381
I1027 19:43:00.399657 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.476381 (* 1 = 0.476381 loss)
I1027 19:43:00.399731 40125 sgd_solver.cpp:138] Iteration 900, lr = 0.0005
I1027 19:46:24.281180 40125 solver.cpp:243] Iteration 950, loss = 0.539018
I1027 19:46:24.281829 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.539018 (* 1 = 0.539018 loss)
I1027 19:46:24.281855 40125 sgd_solver.cpp:138] Iteration 950, lr = 0.0005
I1027 19:49:32.551631 40125 solver.cpp:243] Iteration 1000, loss = 0.473214
I1027 19:49:32.552209 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.473214 (* 1 = 0.473214 loss)
I1027 19:49:32.552237 40125 sgd_solver.cpp:138] Iteration 1000, lr = 0.0005
I1027 19:52:50.376972 40125 solver.cpp:243] Iteration 1050, loss = 0.431143
I1027 19:52:50.379139 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.431143 (* 1 = 0.431143 loss)
I1027 19:52:50.379209 40125 sgd_solver.cpp:138] Iteration 1050, lr = 0.0005
I1027 19:56:01.195624 40125 solver.cpp:243] Iteration 1100, loss = 0.481064
I1027 19:56:01.196440 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.481064 (* 1 = 0.481064 loss)
I1027 19:56:01.196481 40125 sgd_solver.cpp:138] Iteration 1100, lr = 0.0005
I1027 19:59:13.225814 40125 solver.cpp:243] Iteration 1150, loss = 0.482337
I1027 19:59:13.226598 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.482337 (* 1 = 0.482337 loss)
I1027 19:59:13.226625 40125 sgd_solver.cpp:138] Iteration 1150, lr = 0.0005
I1027 20:02:40.781934 40125 solver.cpp:243] Iteration 1200, loss = 0.509106
I1027 20:02:40.783066 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.509106 (* 1 = 0.509106 loss)
I1027 20:02:40.783119 40125 sgd_solver.cpp:138] Iteration 1200, lr = 0.0005
I1027 20:06:02.412256 40125 solver.cpp:243] Iteration 1250, loss = 0.616274
I1027 20:06:02.413035 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.616274 (* 1 = 0.616274 loss)
I1027 20:06:02.413111 40125 sgd_solver.cpp:138] Iteration 1250, lr = 0.0005
I1027 20:09:18.045295 40125 solver.cpp:243] Iteration 1300, loss = 0.499624
I1027 20:09:18.047251 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.499624 (* 1 = 0.499624 loss)
I1027 20:09:18.047293 40125 sgd_solver.cpp:138] Iteration 1300, lr = 0.0005
I1027 20:12:39.831995 40125 solver.cpp:243] Iteration 1350, loss = 0.572254
I1027 20:12:39.832415 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.572254 (* 1 = 0.572254 loss)
I1027 20:12:39.832437 40125 sgd_solver.cpp:138] Iteration 1350, lr = 0.0005
I1027 20:15:54.288527 40125 solver.cpp:243] Iteration 1400, loss = 0.542169
I1027 20:15:54.289294 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.542169 (* 1 = 0.542169 loss)
I1027 20:15:54.289346 40125 sgd_solver.cpp:138] Iteration 1400, lr = 0.0005
I1027 20:19:12.328840 40125 solver.cpp:243] Iteration 1450, loss = 0.525927
I1027 20:19:12.329375 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.525927 (* 1 = 0.525927 loss)
I1027 20:19:12.329406 40125 sgd_solver.cpp:138] Iteration 1450, lr = 0.0005
I1027 20:22:40.012749 40125 solver.cpp:243] Iteration 1500, loss = 0.425289
I1027 20:22:40.013283 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.425289 (* 1 = 0.425289 loss)
I1027 20:22:40.013314 40125 sgd_solver.cpp:138] Iteration 1500, lr = 0.0005
I1027 20:26:05.399271 40125 solver.cpp:243] Iteration 1550, loss = 0.477906
I1027 20:26:05.399804 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.477906 (* 1 = 0.477906 loss)
I1027 20:26:05.399832 40125 sgd_solver.cpp:138] Iteration 1550, lr = 0.0005
I1027 20:29:21.355041 40125 solver.cpp:243] Iteration 1600, loss = 0.408673
I1027 20:29:21.355485 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.408673 (* 1 = 0.408673 loss)
I1027 20:29:21.355545 40125 sgd_solver.cpp:138] Iteration 1600, lr = 0.0005
I1027 20:32:53.399884 40125 solver.cpp:243] Iteration 1650, loss = 0.457159
I1027 20:32:53.400288 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.457159 (* 1 = 0.457159 loss)
I1027 20:32:53.400313 40125 sgd_solver.cpp:138] Iteration 1650, lr = 0.0005
I1027 20:36:09.293875 40125 solver.cpp:243] Iteration 1700, loss = 0.458079
I1027 20:36:09.294900 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.458079 (* 1 = 0.458079 loss)
I1027 20:36:09.294927 40125 sgd_solver.cpp:138] Iteration 1700, lr = 0.0005
I1027 20:39:27.520880 40125 solver.cpp:243] Iteration 1750, loss = 0.405225
I1027 20:39:27.521296 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.405225 (* 1 = 0.405225 loss)
I1027 20:39:27.521338 40125 sgd_solver.cpp:138] Iteration 1750, lr = 0.0005
I1027 20:42:57.621794 40125 solver.cpp:243] Iteration 1800, loss = 0.553464
I1027 20:42:57.623126 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.553464 (* 1 = 0.553464 loss)
I1027 20:42:57.623164 40125 sgd_solver.cpp:138] Iteration 1800, lr = 0.0005
I1027 20:46:26.352164 40125 solver.cpp:243] Iteration 1850, loss = 0.447804
I1027 20:46:26.352582 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.447804 (* 1 = 0.447804 loss)
I1027 20:46:26.352613 40125 sgd_solver.cpp:138] Iteration 1850, lr = 0.0005
I1027 20:49:38.821547 40125 solver.cpp:243] Iteration 1900, loss = 0.514442
I1027 20:49:38.822338 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.514442 (* 1 = 0.514442 loss)
I1027 20:49:38.822366 40125 sgd_solver.cpp:138] Iteration 1900, lr = 0.0005
I1027 20:53:07.020422 40125 solver.cpp:243] Iteration 1950, loss = 0.543848
I1027 20:53:07.020939 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.543848 (* 1 = 0.543848 loss)
I1027 20:53:07.021024 40125 sgd_solver.cpp:138] Iteration 1950, lr = 0.0005
I1027 20:56:26.961344 40125 solver.cpp:358] Iteration 2000, Testing net (#0)
I1027 20:56:26.962011 40125 net.cpp:693] Ignoring source layer input-data
I1027 20:56:30.323168 40125 solver.cpp:425]     Test net output #0: RegressionLoss = 0.781882 (* 1 = 0.781882 loss)
I1027 20:56:33.926120 40125 solver.cpp:243] Iteration 2000, loss = 0.376115
I1027 20:56:33.926481 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.376115 (* 1 = 0.376115 loss)
I1027 20:56:33.926525 40125 sgd_solver.cpp:138] Iteration 2000, lr = 0.0005
I1027 20:59:52.662832 40125 solver.cpp:243] Iteration 2050, loss = 0.479079
I1027 20:59:52.663543 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.479079 (* 1 = 0.479079 loss)
I1027 20:59:52.663573 40125 sgd_solver.cpp:138] Iteration 2050, lr = 0.0005
I1027 21:03:29.071667 40125 solver.cpp:243] Iteration 2100, loss = 0.447936
I1027 21:03:29.072108 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.447936 (* 1 = 0.447936 loss)
I1027 21:03:29.072160 40125 sgd_solver.cpp:138] Iteration 2100, lr = 0.0005
I1027 21:06:49.601464 40125 solver.cpp:243] Iteration 2150, loss = 0.383054
I1027 21:06:49.602635 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.383054 (* 1 = 0.383054 loss)
I1027 21:06:49.602671 40125 sgd_solver.cpp:138] Iteration 2150, lr = 0.0005
I1027 21:10:04.722553 40125 solver.cpp:243] Iteration 2200, loss = 0.466208
I1027 21:10:04.723124 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.466208 (* 1 = 0.466208 loss)
I1027 21:10:04.723170 40125 sgd_solver.cpp:138] Iteration 2200, lr = 0.0005
I1027 21:13:29.057858 40125 solver.cpp:243] Iteration 2250, loss = 0.482966
I1027 21:13:29.058956 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.482966 (* 1 = 0.482966 loss)
I1027 21:13:29.058992 40125 sgd_solver.cpp:138] Iteration 2250, lr = 0.0005
I1027 21:16:51.184944 40125 solver.cpp:243] Iteration 2300, loss = 0.468623
I1027 21:16:51.185421 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.468623 (* 1 = 0.468623 loss)
I1027 21:16:51.185474 40125 sgd_solver.cpp:138] Iteration 2300, lr = 0.0005
I1027 21:20:14.272881 40125 solver.cpp:243] Iteration 2350, loss = 0.423895
I1027 21:20:14.273627 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.423895 (* 1 = 0.423895 loss)
I1027 21:20:14.273656 40125 sgd_solver.cpp:138] Iteration 2350, lr = 0.0005
I1027 21:23:34.814790 40125 solver.cpp:243] Iteration 2400, loss = 0.340326
I1027 21:23:34.815152 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.340326 (* 1 = 0.340326 loss)
I1027 21:23:34.815181 40125 sgd_solver.cpp:138] Iteration 2400, lr = 0.0005
I1027 21:26:57.883286 40125 solver.cpp:243] Iteration 2450, loss = 0.387465
I1027 21:26:57.884050 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.387465 (* 1 = 0.387465 loss)
I1027 21:26:57.884078 40125 sgd_solver.cpp:138] Iteration 2450, lr = 0.0005
I1027 21:30:10.087986 40125 solver.cpp:243] Iteration 2500, loss = 0.482003
I1027 21:30:10.088778 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.482003 (* 1 = 0.482003 loss)
I1027 21:30:10.088804 40125 sgd_solver.cpp:138] Iteration 2500, lr = 0.0005
I1027 21:33:26.849447 40125 solver.cpp:243] Iteration 2550, loss = 0.459365
I1027 21:33:26.850065 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.459365 (* 1 = 0.459365 loss)
I1027 21:33:26.850143 40125 sgd_solver.cpp:138] Iteration 2550, lr = 0.0005
I1027 21:36:35.538677 40125 solver.cpp:243] Iteration 2600, loss = 0.413425
I1027 21:36:35.539278 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.413425 (* 1 = 0.413425 loss)
I1027 21:36:35.539319 40125 sgd_solver.cpp:138] Iteration 2600, lr = 0.0005
I1027 21:39:46.372095 40125 solver.cpp:243] Iteration 2650, loss = 0.410995
I1027 21:39:46.373073 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.410995 (* 1 = 0.410995 loss)
I1027 21:39:46.373152 40125 sgd_solver.cpp:138] Iteration 2650, lr = 0.0005
I1027 21:43:04.365742 40125 solver.cpp:243] Iteration 2700, loss = 0.475238
I1027 21:43:04.366798 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.475238 (* 1 = 0.475238 loss)
I1027 21:43:04.366838 40125 sgd_solver.cpp:138] Iteration 2700, lr = 0.0005
I1027 21:46:22.468664 40125 solver.cpp:243] Iteration 2750, loss = 0.489442
I1027 21:46:22.469043 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.489442 (* 1 = 0.489442 loss)
I1027 21:46:22.469074 40125 sgd_solver.cpp:138] Iteration 2750, lr = 0.0005
I1027 21:49:40.186741 40125 solver.cpp:243] Iteration 2800, loss = 0.469793
I1027 21:49:40.187825 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.469793 (* 1 = 0.469793 loss)
I1027 21:49:40.187872 40125 sgd_solver.cpp:138] Iteration 2800, lr = 0.0005
I1027 21:52:57.020655 40125 solver.cpp:243] Iteration 2850, loss = 0.332444
I1027 21:52:57.021136 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.332444 (* 1 = 0.332444 loss)
I1027 21:52:57.021162 40125 sgd_solver.cpp:138] Iteration 2850, lr = 0.0005
I1027 21:56:12.111533 40125 solver.cpp:243] Iteration 2900, loss = 0.445898
I1027 21:56:12.111896 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.445898 (* 1 = 0.445898 loss)
I1027 21:56:12.111918 40125 sgd_solver.cpp:138] Iteration 2900, lr = 0.0005
I1027 21:59:30.914386 40125 solver.cpp:243] Iteration 2950, loss = 0.411086
I1027 21:59:30.914968 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.411086 (* 1 = 0.411086 loss)
I1027 21:59:30.914999 40125 sgd_solver.cpp:138] Iteration 2950, lr = 0.0005
I1027 22:03:01.782541 40125 solver.cpp:243] Iteration 3000, loss = 0.368715
I1027 22:03:01.783028 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.368715 (* 1 = 0.368715 loss)
I1027 22:03:01.783056 40125 sgd_solver.cpp:138] Iteration 3000, lr = 0.0005
I1027 22:06:19.349741 40125 solver.cpp:243] Iteration 3050, loss = 0.34151
I1027 22:06:19.350646 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.34151 (* 1 = 0.34151 loss)
I1027 22:06:19.350673 40125 sgd_solver.cpp:138] Iteration 3050, lr = 0.0005
I1027 22:09:38.861886 40125 solver.cpp:243] Iteration 3100, loss = 0.402079
I1027 22:09:38.862732 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.402079 (* 1 = 0.402079 loss)
I1027 22:09:38.862761 40125 sgd_solver.cpp:138] Iteration 3100, lr = 0.0005
I1027 22:12:58.983877 40125 solver.cpp:243] Iteration 3150, loss = 0.478885
I1027 22:12:58.984359 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.478885 (* 1 = 0.478885 loss)
I1027 22:12:58.984400 40125 sgd_solver.cpp:138] Iteration 3150, lr = 0.0005
I1027 22:16:20.294250 40125 solver.cpp:243] Iteration 3200, loss = 0.400523
I1027 22:16:20.295047 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.400523 (* 1 = 0.400523 loss)
I1027 22:16:20.295073 40125 sgd_solver.cpp:138] Iteration 3200, lr = 0.0005
I1027 22:19:44.567392 40125 solver.cpp:243] Iteration 3250, loss = 0.470145
I1027 22:19:44.567950 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.470145 (* 1 = 0.470145 loss)
I1027 22:19:44.567996 40125 sgd_solver.cpp:138] Iteration 3250, lr = 0.0005
I1027 22:23:17.457124 40125 solver.cpp:243] Iteration 3300, loss = 0.426162
I1027 22:23:17.457871 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.426162 (* 1 = 0.426162 loss)
I1027 22:23:17.457942 40125 sgd_solver.cpp:138] Iteration 3300, lr = 0.0005
I1027 22:26:34.103744 40125 solver.cpp:243] Iteration 3350, loss = 0.418761
I1027 22:26:34.104185 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.418761 (* 1 = 0.418761 loss)
I1027 22:26:34.104228 40125 sgd_solver.cpp:138] Iteration 3350, lr = 0.0005
I1027 22:29:42.446976 40125 solver.cpp:243] Iteration 3400, loss = 0.451267
I1027 22:29:42.467775 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.451267 (* 1 = 0.451267 loss)
I1027 22:29:42.467864 40125 sgd_solver.cpp:138] Iteration 3400, lr = 0.0005
I1027 22:32:59.795886 40125 solver.cpp:243] Iteration 3450, loss = 0.541807
I1027 22:32:59.796308 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.541807 (* 1 = 0.541807 loss)
I1027 22:32:59.796332 40125 sgd_solver.cpp:138] Iteration 3450, lr = 0.0005
I1027 22:36:17.095434 40125 solver.cpp:243] Iteration 3500, loss = 0.441687
I1027 22:36:17.096222 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.441687 (* 1 = 0.441687 loss)
I1027 22:36:17.096274 40125 sgd_solver.cpp:138] Iteration 3500, lr = 0.0005
I1027 22:39:48.027217 40125 solver.cpp:243] Iteration 3550, loss = 0.375711
I1027 22:39:48.027940 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.375711 (* 1 = 0.375711 loss)
I1027 22:39:48.027971 40125 sgd_solver.cpp:138] Iteration 3550, lr = 0.0005
I1027 22:43:52.900394 40125 solver.cpp:243] Iteration 3600, loss = 0.568566
I1027 22:43:52.901293 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.568566 (* 1 = 0.568566 loss)
I1027 22:43:52.901363 40125 sgd_solver.cpp:138] Iteration 3600, lr = 0.0005
I1027 22:47:59.695848 40125 solver.cpp:243] Iteration 3650, loss = 0.337012
I1027 22:47:59.696405 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.337012 (* 1 = 0.337012 loss)
I1027 22:47:59.696436 40125 sgd_solver.cpp:138] Iteration 3650, lr = 0.0005
I1027 22:52:22.561673 40125 solver.cpp:243] Iteration 3700, loss = 0.415337
I1027 22:52:22.562861 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.415337 (* 1 = 0.415337 loss)
I1027 22:52:22.562925 40125 sgd_solver.cpp:138] Iteration 3700, lr = 0.0005
I1027 22:56:21.167866 40125 solver.cpp:243] Iteration 3750, loss = 0.450729
I1027 22:56:21.168638 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.450729 (* 1 = 0.450729 loss)
I1027 22:56:21.168663 40125 sgd_solver.cpp:138] Iteration 3750, lr = 0.0005
I1027 23:00:21.681150 40125 solver.cpp:243] Iteration 3800, loss = 0.43163
I1027 23:00:21.681887 40125 solver.cpp:259]     Train net output #0: RegressionLoss = 0.43163 (* 1 = 0.43163 loss)
I1027 23:00:21.681922 40125 sgd_solver.cpp:138] Iteration 3800, lr = 0.0005
